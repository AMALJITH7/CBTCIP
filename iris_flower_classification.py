# -*- coding: utf-8 -*-
"""IRIS FLOWER CLASSIFICATION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sso0--iTAw23WiihNMVOYWXuyicR30HV
"""

import numpy as np
import pandas as pd

data= pd.read_csv('/content/drive/MyDrive/Cipherbytes/Iris Flower - Iris.csv')
data

data.isnull().sum()

data.info()

data.describe()

"""# Histogram representation."""

import seaborn as sns
data.hist(figsize=(10,10))

"""# Checking outliers."""

sns.boxplot(data['SepalLengthCm'])

sns.boxplot(data['PetalLengthCm'])

sns.boxplot(data['PetalWidthCm'])

sns.boxplot(data['SepalWidthCm'])

"""# 'SepalWidthCm' column contain some outliers."""

data.shape

max_threshold = data['SepalWidthCm'].quantile(0.95)
max_threshold

data[data['SepalWidthCm']>max_threshold]

min_threshold = data['SepalWidthCm'].quantile(0.05)
min_threshold

data[data['SepalWidthCm']<min_threshold]

newdata=data[(data['SepalWidthCm']<max_threshold) & (data['SepalWidthCm']>min_threshold)]
newdata

newdata.shape

sns.boxplot(newdata['SepalWidthCm'])

"""# We removed all the outliers from 'SepalWidthCm' column from the data. Now our data get cleaned."""

x = newdata.drop(["Species","Id"], axis=1)
y = newdata["Species"]

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x, y,test_size=0.3,random_state=0)

x_train.head()

y_train.head()

from sklearn.feature_selection import mutual_info_classif #determine the mutual information
mutual_info=mutual_info_classif(x_train,y_train)
mutual_info

mutual_info=pd.Series(mutual_info)
mutual_info.index=x_train.columns
mutual_info.sort_values(ascending=False)

import matplotlib.pyplot as plt
mutual_info.sort_values(ascending=False).plot.bar(figsize=(20,8)) # plot the ordered mutual info

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Initialize classifiers
classifiers = {
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "SVM": SVC(),
    "KNN": KNeighborsClassifier(),
    "Logistic Regression": LogisticRegression()
}

# Train and evaluate classifiers
results = {}
for name, clf in classifiers.items():
    clf.fit(x_train, y_train)
    y_pred = clf.predict(x_test)
    accuracy = accuracy_score(y_test, y_pred)*100
    precision = precision_score(y_test, y_pred, average='weighted')*100
    recall = recall_score(y_test, y_pred, average='weighted')*100
    f1 = f1_score(y_test, y_pred, average='weighted')*100
    cm = confusion_matrix(y_test, y_pred)
    results[name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-score': f1, 'Confusion Matrix': cm}

# Print results
for name, result in results.items():
    print(f"Classifier: {name}")
    print(f"Accuracy: {result['Accuracy']:.2f}%")
    print(f"Precision: {result['Precision']:.2f}%")
    print(f"Recall: {result['Recall']:.2f}%")
    print(f"F1-score: {result['F1-score']:.2f}%")
    print(f"Confusion Matrix:\n{result['Confusion Matrix']}")
    print("---------------------------------------------")

"""# KNN has higher accuracy compared to other classifiers. So choose KNN asa machine learning model for classification."""

from sklearn.model_selection import GridSearchCV
param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'p': [1, 2]  # Distance metric: 1 for Manhattan distance, 2 for Euclidean distance
}

# Initialize logistic RandomForestClassifier
knn = KNeighborsClassifier()
grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(x_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_
print(f"Best Parameters: {best_params}")
print(f"Best Score: {best_score}")

knn.fit(x_train, y_train)

y_pred = knn.predict(x_test)
rf_accuracy = accuracy_score(y_test, y_pred)
rf_precision = precision_score(y_test, y_pred, average='weighted')
rf_recall = recall_score(y_test, y_pred, average='weighted')
rf_f1 = f1_score(y_test, y_pred, average='weighted')
rf_cm = confusion_matrix(y_test, y_pred)

print(f"RandomForestClassifiers has an accuracy of: {round(rf_accuracy * 100, 2)}%")
print(f"RandomForestClassifiers has an precision of: {round(rf_precision * 100, 2)}%")
print(f"RandomForestClassifiers has an recall of: {round(rf_recall * 100, 2)}%")
print(f"RandomForestClassifiers has an f1 score of: {round(rf_f1 * 100, 2)}%")
print(f"Confusion Matrix:\n{rf_cm}")

"""# Check our classification is correct using the same dataset removing the 'Species' column from it."""

test=data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
test

prediction=knn.predict(test)
prediction

"""# Our Model prediction is correct, it predicts the exact species name of the flower when KNN classifiere is used."""